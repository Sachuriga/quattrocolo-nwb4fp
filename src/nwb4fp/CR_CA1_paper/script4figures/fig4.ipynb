{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b563e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"Q:/sachuriga/Sachuriga_Python/quattrocolo-nwb4fp/src\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from neurochat.nc_data import NData\n",
    "from neurochat.nc_spike import NSpike\n",
    "from neurochat.nc_spatial import NSpatial\n",
    "import neurochat.nc_plot as nc_plot\n",
    "from neurochat.nc_lfp import NLfp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pynwb import NWBHDF5IO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pynapple as nap\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import sys\n",
    "import nwb4fp.analyses.maps as mapp\n",
    "from nwb4fp.analyses.examples.tracking_plot import plot_ratemap_ax,plot_path\n",
    "from nwb4fp.analyses.fields import separate_fields_by_laplace, separate_fields_by_dilation,find_peaks,separate_fields_by_laplace_of_gaussian,calculate_field_centers,distance_to_edge_function, remove_fields_by_area, map_pass_to_unit_circle,which_field,compute_crossings\n",
    "from elephant.statistics import time_histogram, instantaneous_rate\n",
    "from nwb4fp.analyses import maps\n",
    "from nwb4fp.analyses.data import pos2speed,speed_filtered_spikes,load_speed_fromNWB,load_units_fromNWB,get_filed_num,unit_location_ch,calculate_spatial_coherence,calculate_spatial_stability,coherence, find_run_indices\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import ast\n",
    "import pandas as pd\n",
    "# df = pd.read_csv(rf'{ephys_path}\\{animal}\\{animal}_{day}_unitmatchResults\\MatchTable.csv')\n",
    "# df = pd.read_csv(r'S:\\Sachuriga\\Ephys_Recording\\CR_CA1/65165/65165_2023-07-10_unitmatchResults/MatchTable.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c00b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(7.2, 3.7),dpi=1200)\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "gs = gridspec.GridSpec(2, 4, height_ratios=[1, 1], width_ratios=[0.9, 0.9, 0.9, 0.9])  # First row taller\n",
    "plt.rcParams.update({\n",
    "    'axes.labelpad': -0.1,\n",
    "    'ytick.major.pad': -0.1,\n",
    "    'xtick.major.pad': -0.1,\n",
    "    'ytick.major.size': 2,\n",
    "    'xtick.major.size': 2\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "ax1_2 = fig.add_subplot(gs[0, 1])\n",
    "ax1_3 = fig.add_subplot(gs[0, 2])\n",
    "ax1_4 = fig.add_subplot(gs[0, 3])\n",
    "\n",
    "ax2_1 = fig.add_subplot(gs[1, 0], projection='3d')\n",
    "ax2_2 = fig.add_subplot(gs[1, 1])\n",
    "ax2_3 = fig.add_subplot(gs[1, 2])\n",
    "ax2_4 = fig.add_subplot(gs[1, 3])\n",
    "\n",
    "\n",
    "pairs_df=pd.read_pickle(r\"Q:\\sachuriga\\CR_CA1_paper\\tables/Remapping_ratemaps_unsmoothed.pkl\")\n",
    "# Assuming your data is loaded in pairs_df\n",
    "df = pairs_df[(pairs_df['cell_type1']==\"pyramidal\") \n",
    "              & (pairs_df['cell_type2']==\"pyramidal\")]\n",
    "\n",
    "\n",
    "# df = pairs_df[(pairs_df['neuron location 1']==\"superficial\") \n",
    "#               & (pairs_df['neuron location 2']==\"superficial\")]\n",
    "\n",
    "\n",
    "# Define control and experimental animal_ids\n",
    "control_ids = ['65165', '65091', '63383', '66539', '65622']\n",
    "exp_ids = ['65588', '63385', '66538', '66537', '66922']\n",
    "\n",
    "# Split into three groups based on Session1 and Session2\n",
    "ab_group = df[(df['Session1'] == 1.0) & (df['Session2'] == 2.0)]  # AB group\n",
    "ba_group = df[(df['Session1'] == 2.0) & (df['Session2'] == 3.0)]  # BA' group\n",
    "aa_group = df[(df['Session1'] == 1.0) & (df['Session2'] == 3.0)]  # AA' group\n",
    "\n",
    "# Split into control and experimental groups\n",
    "control_ab = ab_group[ab_group['animal_id'].isin(control_ids)]\n",
    "control_ba = ba_group[ba_group['animal_id'].isin(control_ids)]\n",
    "control_aa = aa_group[aa_group['animal_id'].isin(control_ids)]\n",
    "\n",
    "exp_ab = ab_group[ab_group['animal_id'].isin(exp_ids)]\n",
    "exp_ba = ba_group[ba_group['animal_id'].isin(exp_ids)]\n",
    "exp_aa = aa_group[aa_group['animal_id'].isin(exp_ids)]\n",
    "\n",
    "# Prepare plotting data\n",
    "plot_data = []\n",
    "\n",
    "# Add control group data\n",
    "for group_num, group_df in enumerate([control_ab, control_ba, control_aa], 1):\n",
    "    for corr_value in group_df['Corr']:\n",
    "        plot_data.append({\n",
    "            'Corr': corr_value,\n",
    "            'Group': f'Group{group_num}',\n",
    "            'Condition': 'Control'\n",
    "        })\n",
    "\n",
    "# Add experimental group data\n",
    "for group_num, group_df in enumerate([exp_ab, exp_ba, exp_aa], 1):\n",
    "    for corr_value in group_df['Corr']:\n",
    "        plot_data.append({\n",
    "            'Corr': corr_value,\n",
    "            'Group': f'Group{group_num}',\n",
    "            'Condition': 'Experimental'\n",
    "        })\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "axes_cdf = [ax1_2,ax1_3,ax1_4]\n",
    "\n",
    "group_labels = ['S1=1.0, S2=2.0', 'S1=2.0, S2=3.0', 'S1=1.0, S2=3.0']\n",
    "alternatives = ['two-sided', 'two-sided', 'two-sided']  # AB: Exp > Control, BA': two-sided, AA': Exp < Control\n",
    "\n",
    "# Perform statistical tests and create plots\n",
    "for i, group in enumerate(['Group1', 'Group2', 'Group3']):\n",
    "    # Get data for current group\n",
    "    control_data = plot_df[(plot_df['Group'] == group) & (plot_df['Condition'] == 'Control')]['Corr']\n",
    "    exp_data = plot_df[(plot_df['Group'] == group) & (plot_df['Condition'] == 'Experimental')]['Corr']\n",
    "    \n",
    "    # Perform KS test\n",
    "    ks_statistic, ks_p_value = ks_2samp(control_data, exp_data, alternative=alternatives[i])\n",
    "    # Perform Mann-Whitney U test\n",
    "    mw_statistic, mw_p_value = mannwhitneyu(control_data, exp_data, alternative=alternatives[i])\n",
    "    print(f\"Group{i+1} ({group_labels[i]}): KS p-value = {ks_p_value:.4f}, MW p-value = {mw_p_value:.4f}\")\n",
    "    # Plot CDF (second row)\n",
    "    sns.ecdfplot(data=control_data, ax=axes_cdf[i], color='blue', label='Control')\n",
    "    sns.ecdfplot(data=exp_data, ax=axes_cdf[i], color='red', label='Experimental')\n",
    "    \n",
    "\n",
    "    # Customize CDF subplot\n",
    "    #axes_cdf[i].set_title(f'Mann-Whitney p = {mw_p_value:.3f}', fontsize=10)\n",
    "    axes_cdf[i].set_xlabel('Spatial correlation')\n",
    "    axes_cdf[i].set_ylabel('% Neuron')\n",
    "    axes_cdf[i].legend()\n",
    "    axes_cdf[i].grid(False)\n",
    "    axes_cdf[i].set_aspect('auto')\n",
    "    axes_cdf[i].spines['right'].set_visible(False)\n",
    "    axes_cdf[i].spines['top'].set_visible(False)\n",
    "    axes_cdf[i].legend().set_visible(False)\n",
    "    axes_cdf[i].set_xlim([-0.4, 1])\n",
    "\n",
    "        # Calculate means and SEM\n",
    "    control_mean = np.mean(control_data)\n",
    "    exp_mean = np.mean(exp_data)\n",
    "    control_sem = stats.sem(control_data)\n",
    "    exp_sem = stats.sem(exp_data)\n",
    "\n",
    "    # Create inset axes for bar plot in top-right corner\n",
    "    inset_ax = inset_axes(axes_cdf[i], width=\"80%\", height=\"80%\", loc='lower right',\n",
    "                     bbox_to_anchor=(0.7, 0, 0.3, 0.3),  # 4-tuple: x0, y0, width, height\n",
    "                     bbox_transform=axes_cdf[i].transAxes)\n",
    "    \n",
    "    inset_ax.bar([0.2, 0.8], [control_mean, exp_mean], yerr=[control_sem, exp_sem], \n",
    "                 color=['blue', 'red'], alpha=0.9, width=0.45, capsize=2)\n",
    "    inset_ax.set_xticks([])\n",
    "    inset_ax.set_yticks([])\n",
    "    #inset_ax.set_xticklabels(['CRs +', 'CRs -'], fontsize=8)\n",
    "    #inset_ax.set_ylabel('Mean', fontsize=8)\n",
    "    inset_ax.tick_params(axis='both', labelsize=6)\n",
    "    inset_ax.spines['top'].set_visible(False)\n",
    "    inset_ax.spines['right'].set_visible(False)\n",
    "    inset_ax.spines['left'].set_visible(False)\n",
    "    inset_ax.set_ylim([0, 0.6])\n",
    "\n",
    "\n",
    "all_rates = np.load(r\"Q:\\sachuriga\\CR_CA1_paper\\tables/population_vectors.npy\")\n",
    "rate_maps = np.array(all_rates)\n",
    "rate_maps=rate_maps[1:8,:,:]\n",
    "n_cells, height, width = rate_maps.shape\n",
    "position=(18, 5)\n",
    "x_pos, y_pos = position\n",
    "ax= ax2_1\n",
    "\n",
    "# Plot each rate map as a 2D heatmap at different Z levels\n",
    "for z in range(n_cells + 2):\n",
    "\n",
    "    if z < n_cells:\n",
    "        # Create a grid for the heatmap\n",
    "        X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "        # Normalize the rate map for colormap\n",
    "        rate_map = rate_maps[z]\n",
    "        # Create an RGBA image for the heatmap\n",
    "        cmap = plt.cm.hot(rate_map / np.max(rate_map))\n",
    "        # Plot the heatmap as a collection of colored squares\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                color = cmap[i, j]\n",
    "                ax.bar3d(j, i, z, 1, 1, 0, color=color, shade=False, alpha=0.6)\n",
    "\n",
    "    if z == n_cells+1:\n",
    "        X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "        # Normalize the rate map for colormap\n",
    "        rate_map = rate_maps[6]\n",
    "        # Create an RGBA image for the heatmap\n",
    "        cmap = plt.cm.hot(rate_map / np.max(rate_map))\n",
    "        # Plot the heatmap as a collection of colored squares\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                color = cmap[i, j]\n",
    "                ax.bar3d(j, i, -3, 1, 1, 0, color=color, shade=False, alpha=0.6)\n",
    "\n",
    "# Add a vertical line at (x=15, y=15)\n",
    "ax.plot3D([x_pos, x_pos], [y_pos, y_pos],[-4, 8], color='blue')\n",
    "\n",
    "# Set labels and limits\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Cell Index')\n",
    "ax.set_zlim(-2, n_cells-1)\n",
    "ax.set_xlim(0, width)\n",
    "ax.set_ylim(0, height)\n",
    "\n",
    "# Customize z-axis ticks to show \"1, 2, 3, ..., n\"\n",
    "z_ticks = np.arange(n_cells)\n",
    "z_labels = [str(i+1) for i in range(n_cells)]\n",
    "z_labels[-1] = 'n'  # Replace the last label with 'n'\n",
    "ax.set_zticks(z_ticks)\n",
    "ax.set_zticklabels(z_labels)\n",
    "\n",
    "# Adjust the viewing angle for a more parallel perspective\n",
    "ax.view_init(elev=10, azim=45)  # Elevation = 10째, Azimuth = 45째\n",
    "ax.axis('off')\n",
    "ax.grid('off')\n",
    "\n",
    "# Assume processed_df_normalized and population_vector_correlation are defined\n",
    "patterns = [(\"A\", \"B\"), (\"B\", \"C\"), (\"A\", \"C\")]\n",
    "\n",
    "# Set up figure with subplots (2 rows, 3 columns)\n",
    "axes = [ax2_2,ax2_3,ax2_4]\n",
    "\n",
    "for idx, (s1, s2) in enumerate(patterns):\n",
    "    # Filter rows where both s1 and s2 have values (not null)\n",
    "    filtered_df = processed_df_normalized.dropna(subset=[s1, s2])\n",
    "    control_df = filtered_df[filtered_df['group'] == 'control']\n",
    "    exp_df = filtered_df[filtered_df['group'] == 'exp']\n",
    "\n",
    "    def to_3d_array(df, column, i=2, j=2):\n",
    "        values = df[column].values\n",
    "        n = len(values)\n",
    "        return np.array([np.full((i, j), val) for val in values])\n",
    "\n",
    "    i, j = 20, 20\n",
    "    control_a_3d = to_3d_array(control_df, s1, i, j)\n",
    "    control_b_3d = to_3d_array(control_df, s2, i, j)\n",
    "    exp_a_3d = to_3d_array(exp_df, s1, i, j)\n",
    "    exp_b_3d = to_3d_array(exp_df, s2, i, j)\n",
    "    control_corr = population_vector_correlation(control_a_3d, control_b_3d, full='off')\n",
    "    exp_corr = population_vector_correlation(exp_a_3d, exp_b_3d, full='off')\n",
    "\n",
    "    if control_corr is None or exp_corr is None:\n",
    "        print(f\"Not enough neurons (<5) to compute correlations for {s1} and {s2}. Adjust your data.\")\n",
    "        continue\n",
    "\n",
    "    control_corr_flat = control_corr.flatten()\n",
    "    exp_corr_flat = exp_corr.flatten()\n",
    "\n",
    "    bin_size = 0.05\n",
    "    bins = np.arange(min(control_corr_flat.min(), exp_corr_flat.min()), \n",
    "                     max(control_corr_flat.max(), exp_corr_flat.max()) + bin_size, \n",
    "                     bin_size)\n",
    "\n",
    "    # Compute histograms (counts, not density)\n",
    "    control_hist, control_bin_edges = np.histogram(control_corr_flat, bins=bins)\n",
    "    exp_hist, exp_bin_edges = np.histogram(exp_corr_flat, bins=bins)\n",
    "    \n",
    "    # Calculate fractions\n",
    "    control_frac = control_hist / control_hist.sum()\n",
    "    exp_frac = exp_hist / exp_hist.sum()\n",
    "\n",
    "    # Calculate bin centers\n",
    "    control_bin_centers = (control_bin_edges[:-1] + control_bin_edges[1:]) / 2\n",
    "    exp_bin_centers = (exp_bin_edges[:-1] + exp_bin_edges[1:]) / 2\n",
    "\n",
    "    # Create pseudo-datasets by multiplying bin counts with bin centers\n",
    "    control_pseudo = []\n",
    "    exp_pseudo = []\n",
    "    \n",
    "    for count, center in zip(control_hist, control_bin_centers):\n",
    "        control_pseudo.extend([center] * int(count))\n",
    "    \n",
    "    for count, center in zip(exp_hist, exp_bin_centers):\n",
    "        exp_pseudo.extend([center] * int(count))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    control_pseudo = np.array(control_pseudo)\n",
    "    exp_pseudo = np.array(exp_pseudo)\n",
    "\n",
    "    # Compute CDFs for plotting (using original data)\n",
    "    control_hist_density, _ = np.histogram(control_corr_flat, bins=bins, density=True)\n",
    "    exp_hist_density, _ = np.histogram(exp_corr_flat, bins=bins, density=True)\n",
    "    control_cdf = np.cumsum(control_hist_density) / np.sum(control_hist_density)\n",
    "    exp_cdf = np.cumsum(exp_hist_density) / np.sum(exp_hist_density)\n",
    "\n",
    "\n",
    "    # Plot CDFs\n",
    "    axes[idx].plot(control_bin_edges[:-1], control_cdf, label=f'Control ({s1}, {s2})', \n",
    "                      color='blue', linestyle='-', drawstyle='steps-post')\n",
    "    axes[idx].plot(exp_bin_edges[:-1], exp_cdf, label=f'Exp ({s1}, {s2})', \n",
    "                      color='red', linestyle='-', drawstyle='steps-post')\n",
    "    axes[idx].set_xlabel('Correlation of PV')\n",
    "    axes[idx].set_ylabel(\"% Spatial bins\")\n",
    "    axes[idx].legend().set_visible(False)\n",
    "    axes[idx].spines['top'].set_visible(False)\n",
    "    axes[idx].spines['right'].set_visible(False)\n",
    "    axes[idx].set_aspect('auto')\n",
    "    axes[idx].set_xlim([-0.2, 1])\n",
    "\n",
    "    # Calculate means and SEM\n",
    "    control_mean = np.mean(control_corr_flat)\n",
    "    exp_mean = np.mean(exp_corr_flat)\n",
    "    control_sem = stats.sem(control_corr_flat)\n",
    "    exp_sem = stats.sem(exp_corr_flat)\n",
    "\n",
    "    # Create inset axes for bar plot in top-right corner\n",
    "    inset_ax = inset_axes(axes[idx], width=\"100%\", height=\"100%\", loc='lower right',\n",
    "                     bbox_to_anchor=(0.7, 0.05, 0.3, 0.3),  # 4-tuple: x0, y0, width, height\n",
    "                     bbox_transform=axes[idx].transAxes)\n",
    "    \n",
    "    inset_ax.bar([0.2, 0.8], [control_mean, exp_mean], yerr=[control_sem, exp_sem], \n",
    "                 color=['blue', 'red'], alpha=0.9, width=0.45, capsize=3, error_kw={'elinewidth': 0.1})\n",
    "    inset_ax.set_xticks([0.2, 0.8])\n",
    "    inset_ax.set_xticklabels(['CRs +', 'CRs -'])\n",
    "    #inset_ax.set_ylabel('Mean Corr')\n",
    "    inset_ax.tick_params(axis='both')\n",
    "    inset_ax.spines['top'].set_visible(False)\n",
    "    inset_ax.spines['right'].set_visible(False)\n",
    "    inset_ax.set_ylim([0, 0.6])\n",
    "    inset_ax.set_xticks([])\n",
    "    inset_ax.set_yticks([])\n",
    "    #inset_ax.set_xticklabels(['CRs +', 'CRs -'], fontsize=8)\n",
    "    #inset_ax.set_ylabel('Mean', fontsize=8)\n",
    "    inset_ax.tick_params(axis='both')\n",
    "    inset_ax.spines['top'].set_visible(False)\n",
    "    inset_ax.spines['right'].set_visible(False)\n",
    "    inset_ax.spines['left'].set_visible(False)\n",
    "\n",
    "    u_stat, p_value_u = stats.mannwhitneyu(control_corr_flat, exp_corr_flat, alternative='two-sided')\n",
    "    ks_stat, p_value_ks = stats.ks_2samp(control_pseudo, exp_pseudo)\n",
    "\n",
    "    y_max = inset_ax.get_ylim()[1]\n",
    "    bar_height = y_max * 0.1  # Adjust this value to position the bar above the plot\n",
    "    x_positions = [0.2, 0.8]  # Adjusted positions for 'Control' and 'Experimental' groups\n",
    "    if (p_value_u < 0.05) & (p_value_u  > 0.01):\n",
    "        inset_ax.plot([x_positions[0], x_positions[1]], [y_max + bar_height, y_max + bar_height], \n",
    "                            color='black', lw=1.5)\n",
    "        inset_ax.text(x_positions[1]*0.5, y_max + bar_height * 1.1, f'*', ha='center', va='bottom')\n",
    "    elif (p_value_u < 0.01) & (p_value_u  > 0.001):\n",
    "        inset_ax.plot([x_positions[0], x_positions[1]], [y_max + bar_height, y_max + bar_height], \n",
    "                            color='black', lw=1.5)\n",
    "        inset_ax.text(x_positions[1]*0.5, y_max + bar_height * 1.1, f'**', ha='center', va='bottom')\n",
    "    if (p_value_u < 0.001) :\n",
    "        inset_ax.plot([x_positions[0], x_positions[1]], [y_max+ bar_height, y_max+ bar_height], \n",
    "                        color='black', lw=1.5)\n",
    "        inset_ax.text(x_positions[1]*0.6, y_max + bar_height * 1.1, f'***', ha='center', va='bottom')\n",
    "    inset_ax.set_ylim([0, 0.7])\n",
    "    # Statistical tests\n",
    "    u_stat, p_value_u = stats.mannwhitneyu(control_corr_flat, exp_corr_flat, alternative='two-sided')\n",
    "    ks_stat, p_value_ks = stats.ks_2samp(control_pseudo, exp_pseudo)\n",
    "    print(f\"Pattern {s1} vs. {s2}: MW p-value = {p_value_u:.4f}, KS p-value = {p_value_ks:.4f}\")\n",
    "\n",
    "\n",
    "fig.subplots_adjust(top=0.92, bottom=0.08, left=0.1, right=0.95, hspace=0.3, wspace=0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b821fae",
   "metadata": {},
   "outputs": [],
   "source": [
    " y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_dataframe(df, normalize=True):\n",
    "    # Drop rows where ratemaps is NaN/None\n",
    "    df = df.dropna(subset=['rate_maps'])\n",
    "    \n",
    "    # Define control and experimental groups\n",
    "    control_ids = ['65165', '65091', '63383', '66539', '65622']\n",
    "    exp_ids = ['65588', '63385', '66538', '66537', '66922']\n",
    "    # Create new dataframe to store results\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        animal_id = row['animal_id']\n",
    "        ratemaps = row['rate_maps']\n",
    "        \n",
    "        # Determine group\n",
    "        if animal_id in control_ids:\n",
    "            group = 'control'\n",
    "        elif animal_id in exp_ids:\n",
    "            group = 'exp'\n",
    "        \n",
    "        # Create new row dictionary\n",
    "        new_row = {'animal_id': animal_id, 'group': group}\n",
    "        # Handle different ratemaps lengths and optionally normalize\n",
    "        if len(ratemaps) == 3:\n",
    "            # Process each map with optional normalization\n",
    "            if normalize:\n",
    "                new_row['A'] = ratemaps[0] / np.nanmean(ratemaps[0]) if np.nanmean(ratemaps[0]) != 0 else ratemaps[0]\n",
    "                new_row['B'] = ratemaps[1] / np.nanmean(ratemaps[1]) if np.nanmean(ratemaps[1]) != 0 else ratemaps[1]\n",
    "                new_row['C'] = ratemaps[2] / np.nanmean(ratemaps[2]) if np.nanmean(ratemaps[2]) != 0 else ratemaps[2]\n",
    "            else:\n",
    "                new_row['A'] = ratemaps[0]\n",
    "                new_row['B'] = ratemaps[1]\n",
    "                new_row['C'] = ratemaps[2]\n",
    "            \n",
    "        elif len(ratemaps) == 2:\n",
    "            # Process maps with optional normalization\n",
    "            if normalize:\n",
    "                map1 = ratemaps[0] / np.nanmean(ratemaps[0]) if np.nanmean(ratemaps[0]) != 0 else ratemaps[0]\n",
    "                map2 = ratemaps[1] / np.nanmean(ratemaps[1]) if np.nanmean(ratemaps[1]) != 0 else ratemaps[1]\n",
    "            else:\n",
    "                map1 = ratemaps[0]\n",
    "                map2 = ratemaps[1]\n",
    "\n",
    "            # Assign maps based on session info\n",
    "            if 'Session1' in row and 'Session2' in row:\n",
    "                if row['Session1'] == 1.0:\n",
    "                    new_row['A'] = map1\n",
    "                    if row['Session2'] == 2.0:\n",
    "                        new_row['B'] = map2\n",
    "                    elif row['Session2'] == 3.0:\n",
    "                        new_row['C'] = map2\n",
    "                \n",
    "                elif row['Session1'] == 2.0:\n",
    "                    new_row['B'] = map1\n",
    "                    new_row['C'] = map2\n",
    "        \n",
    "        # Add row to result dataframe\n",
    "        result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "pairs_df = pd.read_pickle(r\"Q:\\sachuriga\\CR_CA1_paper\\tables/Remapping_ratemaps_unsmoothed.pkl\")\n",
    "# pairs_df = pairs_df[(pairs_df['cell_type1']==\"pyramidal\") \n",
    "#               & (pairs_df['cell_type2']==\"pyramidal\")]\n",
    "\n",
    "# Process with normalization (default)\n",
    "processed_df_normalized = process_dataframe(pairs_df, normalize=True)\n",
    "print(\"Normalized DataFrame:\")\n",
    "print(processed_df_normalized.head())\n",
    "\n",
    "# Process without normalization\n",
    "processed_df_unnormalized = process_dataframe(pairs_df, normalize=False)\n",
    "print(\"\\nUnnormalized DataFrame:\")\n",
    "print(processed_df_unnormalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def population_vector_correlation(stack1, stack2, full='off', orientation='v'):\n",
    "    \"\"\"\n",
    "    Calculates correlation between population vectors.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    stack1 : ndarray\n",
    "        3D matrix (neurons, x_bins, y_bins) - population vector\n",
    "    stack2 : ndarray\n",
    "        3D matrix (neurons, x_bins, y_bins) - population vector\n",
    "    full : str, optional\n",
    "        'off': returns diagonal elements (2D matrix)\n",
    "        'on': returns full correlation matrices (3D matrix)\n",
    "        'vector': returns correlations as a vector (1D array)\n",
    "        Default is 'vector'\n",
    "    orientation : str, optional\n",
    "        'v': vertical (rows/x-axis), 'h': horizontal (columns/y-axis)\n",
    "        Only used when full='vector'. Default is 'v'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pv_corr : ndarray or None\n",
    "        Correlation values (1D, 2D, or 3D depending on 'full'), or None if insufficient neurons\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs and check neuron count\n",
    "    if stack1.ndim != 3 or stack2.ndim != 3:\n",
    "        raise ValueError(\"Input stacks must be 3D arrays\")\n",
    "    \n",
    "    num_cells1 = stack1.shape[0]\n",
    "    num_cells2 = stack2.shape[0]\n",
    "    \n",
    "    if num_cells1 < 5 or num_cells2 < 5:\n",
    "        return None\n",
    "    \n",
    "    # Get dimensions\n",
    "    num_cells = min(num_cells1, num_cells2)\n",
    "    num_x_bins = min(stack1.shape[1], stack2.shape[1])\n",
    "    num_y_bins = min(stack1.shape[2], stack2.shape[2])\n",
    "    \n",
    "    if num_cells1 != num_cells2:\n",
    "        print(f\"Warning: Population vectors have different neuron counts: {num_cells1} vs {num_cells2}\")\n",
    "    \n",
    "    # Truncate stacks to minimum dimensions\n",
    "    stack1 = stack1[:num_cells, :num_x_bins, :num_y_bins]\n",
    "    stack2 = stack2[:num_cells, :num_x_bins, :num_y_bins]\n",
    "    \n",
    "    # Initialize output based on return format\n",
    "    if full == 'vector':\n",
    "        num_bins = num_x_bins if orientation == 'v' else num_y_bins\n",
    "        pv_corr = np.zeros(num_bins)\n",
    "    elif full == 'off':\n",
    "        pv_corr = np.zeros((num_x_bins, num_y_bins))\n",
    "    elif full == 'on':\n",
    "        pv_corr = np.zeros((num_x_bins, num_y_bins, num_cells))\n",
    "    else:\n",
    "        raise ValueError(\"full must be 'off', 'on', or 'vector'\")\n",
    "    \n",
    "    # Process based on orientation and return format\n",
    "    if full == 'vector':\n",
    "        if orientation == 'v':\n",
    "            for i in range(num_x_bins):\n",
    "                vec1 = stack1[:, i, :].reshape(-1)\n",
    "                vec2 = stack2[:, i, :].reshape(-1)\n",
    "                pv_corr[i] = np.corrcoef(vec1, vec2)[0, 1]\n",
    "        else:  # 'h'\n",
    "            for i in range(num_y_bins):\n",
    "                vec1 = stack1[:, :, i].reshape(-1)\n",
    "                vec2 = stack2[:, :, i].reshape(-1)\n",
    "                pv_corr[i] = np.corrcoef(vec1, vec2)[0, 1]\n",
    "    \n",
    "    else:  # full = 'off' or 'on'\n",
    "        for i in range(num_x_bins):\n",
    "            for j in range(num_y_bins):\n",
    "                vec1 = stack1[:, i, j]\n",
    "                vec2 = stack2[:, i, j]\n",
    "                corr_matrix = np.corrcoef(vec1, vec2)\n",
    "                if full == 'off':\n",
    "                    pv_corr[i, j] = corr_matrix[0, 1]\n",
    "                else:  # 'on'\n",
    "                    pv_corr[i, j, :] = corr_matrix[0, :]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    pv_corr = np.nan_to_num(pv_corr, nan=0)\n",
    "    \n",
    "    return pv_corr\n",
    "\n",
    "# Define control and experimental animal IDs\n",
    "control_ids = ['65165', '65091', '63383', '66539', '65622']\n",
    "exp_ids = ['65588', '63385', '66538', '66537', '66922']\n",
    "\n",
    "# Initialize dictionaries to store PVs for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae915d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_population_vector_3d_heatmap(rate_maps, position,ax=None):\n",
    "    \"\"\"\n",
    "    Plot stacked 2D heatmaps in 3D and a population vector at the specified position.\n",
    "    \n",
    "    Parameters:\n",
    "    rate_maps : ndarray of shape (n_cells, height, width), e.g., (15, 20, 20)\n",
    "    position : tuple (x, y), position to extract the population vector, e.g., (18, 18)\n",
    "    \"\"\"\n",
    "    # Extract dimensions\n",
    "    n_cells, height, width = rate_maps.shape\n",
    "    x_pos, y_pos = position\n",
    "    \n",
    "    \n",
    "    # Plot each rate map as a 2D heatmap at different Z levels\n",
    "    for z in range(n_cells + 2):\n",
    "\n",
    "        if z < n_cells:\n",
    "            # Create a grid for the heatmap\n",
    "            X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "            # Normalize the rate map for colormap\n",
    "            rate_map = rate_maps[z]\n",
    "            # Create an RGBA image for the heatmap\n",
    "            cmap = plt.cm.hot(rate_map / np.max(rate_map))\n",
    "            # Plot the heatmap as a collection of colored squares\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    color = cmap[i, j]\n",
    "                    ax.bar3d(j, i, z, 1, 1, 0, color=color, shade=False, alpha=0.6)\n",
    "\n",
    "        if z == n_cells+1:\n",
    "            X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "            # Normalize the rate map for colormap\n",
    "            rate_map = rate_maps[3]\n",
    "            # Create an RGBA image for the heatmap\n",
    "            cmap = plt.cm.hot(rate_map / np.max(rate_map))\n",
    "            # Plot the heatmap as a collection of colored squares\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    color = cmap[i, j]\n",
    "                    ax.bar3d(j, i, -3, 1, 1, 0, color=color, shade=False, alpha=0.6)\n",
    "\n",
    "    # Add a vertical line at (x=15, y=15)\n",
    "    ax.plot3D([x_pos, x_pos], [y_pos, y_pos],[-4, 8], color='blue')\n",
    "    \n",
    " \n",
    "    # Set labels and limits\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Cell Index')\n",
    "    ax.set_zlim(0, n_cells-1)\n",
    "    ax.set_xlim(0, width)\n",
    "    ax.set_ylim(0, height)\n",
    "    \n",
    "    # Customize z-axis ticks to show \"1, 2, 3, ..., n\"\n",
    "    z_ticks = np.arange(n_cells)\n",
    "    z_labels = [str(i+1) for i in range(n_cells)]\n",
    "    z_labels[-1] = 'n'  # Replace the last label with 'n'\n",
    "    ax.set_zticks(z_ticks)\n",
    "    ax.set_zticklabels(z_labels)\n",
    "    \n",
    "    # Adjust the viewing angle for a more parallel perspective\n",
    "    ax.view_init(elev=10, azim=45)  # Elevation = 10째, Azimuth = 45째\n",
    "    plt.axis('off')\n",
    "    # Save the plot\n",
    "    plt.savefig('population_vector_3d_heatmap_plot.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
